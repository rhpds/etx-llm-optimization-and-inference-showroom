:imagesdir: ../assets/images

[#deploy-rhoai]
# Deploy vLLM on OpenShift AI with kserve and model car

1. Log into your provided OpenShift environment 
URL: https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank].
* Login to OpenShift ({openshift_cluster_admin_username}/{openshift_cluster_admin_password}) 

2. Once in your OpenShift cluster, confirm you're in vllm namespace. If required create vllm project

[source,console,role=execute,subs=attributes+]
----
oc project vllm
----

## Add helm chart repository

We will use a helm chart to deploy our model-car on vLLM in OpenShift AI. This helm chart is designed to be easily reusable and we recommend starting with this base for your deployment. You may also substitute the official Red Hat AI Inference Server image in the helm chart deployment on a standard OpenShift cluster. For the purposes of this training experience, we are using the vllm-kserve image on OpenShift AI.

First, we'll install helm CLI

[source,console,role=execute,subs=attributes+]
----
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4 | bash
----

Run the following commands to add the redhat-ai-services helm chart repository to your local helm client

[source,console,role=execute,subs=attributes+]
----
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
----

[NOTE]
====
You may view the helm chart at this link: https://github.com/redhat-ai-services/helm-charts/tree/main/charts/vllm-kserve or within our workshop repository under etx-llm-optimization-and-inference-showroom/workshop_code/deploy_vllm/vllm-kserve.
====

## Clone the lab repo

. Run the following command to clone the ETX vLLM optimization repo.
+
[source,console,role=execute,subs=attributes+]
----
git clone https://github.com/rhpds/etx-llm-optimization-and-inference-showroom.git
cd etx-llm-optimization-and-inference-showroom
----

[NOTE]
====
You can view the provided custom values file (etx-llm-optimization-and-inference-showroom/workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml):

[source,console,role=execute,subs=attributes+]
----
deploymentMode: RawDeployment

model:
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  args:
    - "--max-model-len=32768"
    - "--enable-auto-tool-choice"
    - "--tool-call-parser=granite"
    - "--chat-template=/app/data/template/tool_chat_template_granite.jinja"
----
====

## Deploy the vLLM KServe chart

Run the following commands to deploy our vLLM-kserve helm chart. The chart version we are using is published, but we will be deploying it from our cloned repository so that we may view files and make any changes if desired.

OPTIONAL: Before deploying, adjust your `values.yaml` file as you desire as described in above sections.

[source,console,role=execute,subs=attributes+]
----
helm install granite-8b-instruct redhat-ai-services/vllm-kserve --version 0.5.11 \
  -f workshop_code/deploy_vllm/vllm_rhoai_custom_1/values.yaml \
  -n vllm
----

## Verify deployment

It will take several minutes for the model deployment to be ready.

Use your preferred method(s) to verify the successful deployment.

* Login to OpenShift AI ({openshift_cluster_admin_username}/{openshift_cluster_admin_password})
* Go to your *vllm Data Science Project. Wait until the model fully deploys (green check) before continuing.

image::granite-deployed-rhoai-1.png[Granite deployed on RHOAI]

[TIP]
====
You may substitute the modelcar for a different model and adjust the arguments as desired using vLLM standard args: https://docs.vllm.ai/en/stable/configuration/engine_args.html#named-arguments

View our available modelcars here: https://quay.io/repository/redhat-ai-services/modelcar-catalog?tab=tags
====

[IMPORTANT]
====
If you choose to use a different model, you must ensure the `--tool-call-parser` and `--chat-template` arguments are configured appropriately for that model.

* `--tool-call-parser`: See available options at https://docs.vllm.ai/en/latest/cli/index.html#-tool-call-parser
* `--chat-template`: See available chat templates at https://docs.vllm.ai/en/stable/features/tool_calling.html
====

=== Understanding tool-call-parser and chat-template alignment

The `--tool-call-parser` and `--chat-template` arguments must align with the model because they define the specific input and output formats that the model was trained on and expects during inference.

*Chat Template (`--chat-template`)*::
A chat template is a Jinja2 template that structures the conversation history into a single string that the model's tokenizer can process. Different models are fine-tuned on different chat formats. For example, a model might expect messages delimited by specific tokens like `[INST]` and `[/INST]` for user turns, or `pass:[<<SYS>>]` and `pass:[<<EOT>>]` for system messages.
+
*Why alignment is crucial:* If you use a chat template during serving that differs from the one used during model training, the model will receive an input string it doesn't recognize as a valid conversation. It won't correctly interpret the roles of the speakers (user, assistant, system) or the boundaries between messages.
+
This misalignment can lead to:
+
* Nonsensical responses - The model might generate garbled or irrelevant text because it's trying to make sense of an unfamiliar input structure
* Failure to follow instructions - It might ignore system prompts or struggle to maintain conversational coherence
* Incorrect tokenization - The tokenizer might produce a different sequence of tokens than intended if the template's special tokens or formatting are inconsistent with what the model expects

*Tool Call Parser (`--tool-call-parser`)*::
For models that support function calling (i.e., generating structured outputs to use external tools), a tool call parser extracts these structured calls (often in JSON format) from the raw text output generated by the model. The model is specifically fine-tuned to produce tool calls in a very particular syntax and format.
+
*Why alignment is crucial:* If the `--tool-call-parser` during serving doesn't match the format the model was trained to produce, the system won't be able to correctly identify and parse the tool calls.
+
This misalignment can lead to:
+
* Parsing failures - The parser might fail to extract any tool calls, even if the model did generate them, because it's looking for a different structure or syntax
* Incorrect tool calls - It might incorrectly parse parts of the model's output as tool calls, leading to errors or unintended actions when those "calls" are executed
* Loss of functionality - The entire tool-use capability of the model becomes effectively unusable if the parser cannot correctly interpret the model's tool-calling output

## Conclusion

We now have our model car deployed and will move on to model evaluation and benchmarking. Stay in the vllm namespace for the next module and subsequent steps.
